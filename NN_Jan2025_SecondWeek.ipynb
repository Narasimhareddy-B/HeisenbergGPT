{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6450159-d7b7-4171-88c6-100ea36aadf1",
   "metadata": {},
   "source": [
    "## 8th January"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b92b9a8",
   "metadata": {},
   "source": [
    "understanding the tokenizer in GPT-1\n",
    "\n",
    "trying to implement the tokenizer in GPT-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069050bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict\n",
    "pairs = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50046b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"hello\": 5, \"world\": 3}\n",
    "new_vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a811fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h e l l o </w>\n",
      "w o r l d </w>\n",
      "{'h e l l o </w>': 5, 'w o r l d </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "for token in vocab.keys():\n",
    "    ntoken = ' '.join(list(token)) + ' </w>'\n",
    "    new_vocab[ntoken] = vocab[token]\n",
    "    print(ntoken)\n",
    "print(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a5435b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, freq in new_vocab.items():\n",
    "    symbols = word.split()\n",
    "    for i in range(len(symbols)-1):\n",
    "        pairs[symbols[i], symbols[i+1]] += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f9ca7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('h', 'e'): 5,\n",
       "             ('e', 'l'): 5,\n",
       "             ('l', 'l'): 5,\n",
       "             ('l', 'o'): 5,\n",
       "             ('o', '</w>'): 5,\n",
       "             ('w', 'o'): 3,\n",
       "             ('o', 'r'): 3,\n",
       "             ('r', 'l'): 3,\n",
       "             ('l', 'd'): 3,\n",
       "             ('d', '</w>'): 3})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "048449c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('h', 'e')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pairs, key=pairs.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947b106",
   "metadata": {},
   "source": [
    "### 9th January"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81dff4",
   "metadata": {},
   "source": [
    "Working on building Byte Pair Encoding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0988c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import nltk, os\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_tokenizer(file_paths: List[str], min_freq: int, merges: int) -> 'BPETokenizer':\n",
    "        vocab = create_vocab(file_paths)\n",
    "        truncate_vocab(vocab, min_freq)\n",
    "        bpe_vocab = create_bpe_vocab(vocab)\n",
    "\n",
    "\n",
    "def create_vocab(file_paths:  List[str]) -> Dict[str, int]:\n",
    "    vocab = defaultdict(int)\n",
    "    for file_path in tqdm(file_paths, desc='Reading files'):\n",
    "        text = open(file_path, 'r', encoding = 'utf-8-sig').read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = wordpunct_tokenize(sentence)\n",
    "            for token in tokens:\n",
    "                vocab[token] +=1\n",
    "    return vocab\n",
    "\n",
    "def truncate_vocab(vocab: Dict[str, int], min_freq: int) -> None:\n",
    "    tokens = list(vocab.keys()) \n",
    "    for token in tokens:\n",
    "        if vocab[token] < min_freq:\n",
    "            del vocab[token]\n",
    "def create_bpe_vocab(vocab: Dict[str, int]):\n",
    "    bpe_vocab = {}\n",
    "    for token in vocab:\n",
    "        new_token = ' '.join(list(token)) + ' </w>'\n",
    "        bpe_vocab[new_token] = vocab[token]\n",
    "    return bpe_vocab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcabd0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data\\\\BreakingBadSeason1.txt', 'Data\\\\BreakingBadSeason2.txt', 'Data\\\\BreakingBadSeason3.txt', 'Data\\\\BreakingBadSeason4.txt', 'Data\\\\BreakingBadSeason5.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 5/5 [00:01<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import nltk, os\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_vocab(file_paths:  List[str]) -> Dict[str, int]:\n",
    "    vocab = defaultdict(int)\n",
    "    for file_path in tqdm(file_paths, desc='Reading files'):\n",
    "        text = open(file_path, 'r', encoding = 'utf-8-sig').read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = wordpunct_tokenize(sentence)\n",
    "            for token in tokens:\n",
    "                vocab[token] +=1\n",
    "    return vocab\n",
    "\n",
    "file_paths = os.listdir('Data')\n",
    "file_paths = [os.path.join('Data', file_path) for file_path in file_paths]\n",
    "print(file_paths)\n",
    "breaking_bad_vocab = create_vocab(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1bcaa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6245\n",
      "[('.', 4685), (',', 3930), (\"'\", 2722), (\":'''\", 2573), (\"''':\", 2378), ('I', 2213), ('you', 1815), ('?', 1710), ('the', 1507), ('to', 1406), ('Walter', 1308), ('s', 1094), ('a', 1046), ('and', 861), ('Jesse', 843), ('it', 810), ('that', 772), ('of', 714), ('t', 671), (\"]''\", 641), ('You', 582), ('is', 577), ('me', 567), ('!', 562), ('in', 553), ('...', 543), (':', 500), (\"''\", 456), ('this', 435), (\"''[\", 434), ('know', 388), ('w', 385), ('your', 382), ('what', 379), ('for', 370), ('my', 363), ('-', 359), ('re', 358), ('he', 355), ('do', 349), ('on', 347), ('just', 325), ('m', 324), ('Hank', 312), ('not', 311), ('we', 308), ('Skyler', 307), ('Saul', 288), ('What', 285), ('<', 281)]\n"
     ]
    }
   ],
   "source": [
    "print(len(breaking_bad_vocab))\n",
    "print(sorted(breaking_bad_vocab.items(), key=lambda x:x[1], reverse=True)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "796e3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_vocab(vocab: Dict[str, int], min_freq: int) -> None:\n",
    "    tokens = list(vocab.keys()) \n",
    "    for token in tokens:\n",
    "        if vocab[token] < min_freq:\n",
    "            del vocab[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ce494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_vocab(vocab=breaking_bad_vocab, min_freq= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64de1520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Skyler', 307), ('we', 308), ('not', 311), ('Hank', 312), ('m', 324), ('just', 325), ('on', 347), ('do', 349), ('he', 355), ('re', 358), ('-', 359), ('my', 363), ('for', 370), ('what', 379), ('your', 382), ('w', 385), ('know', 388), (\"''[\", 434), ('this', 435), (\"''\", 456), (':', 500), ('...', 543), ('in', 553), ('!', 562), ('me', 567), ('is', 577), ('You', 582), (\"]''\", 641), ('t', 671), ('of', 714), ('that', 772), ('it', 810), ('Jesse', 843), ('and', 861), ('a', 1046), ('s', 1094), ('Walter', 1308), ('to', 1406), ('the', 1507), ('?', 1710), ('you', 1815), ('I', 2213), (\"''':\", 2378), (\":'''\", 2573), (\"'\", 2722), (',', 3930), ('.', 4685)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(breaking_bad_vocab.items(), key=lambda x:x[1])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a836e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpe_vocab(vocab: Dict[str, int]):\n",
    "    bpe_vocab = {}\n",
    "    for token in vocab:\n",
    "        new_token = ' '.join(list(token)) + ' </w>'\n",
    "        bpe_vocab[new_token] = vocab[token]\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8100eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S k y l e r </w>', 307), ('w e </w>', 308), ('n o t </w>', 311), ('H a n k </w>', 312), ('m </w>', 324), ('j u s t </w>', 325), ('o n </w>', 347), ('d o </w>', 349), ('h e </w>', 355), ('r e </w>', 358), ('- </w>', 359), ('m y </w>', 363), ('f o r </w>', 370), ('w h a t </w>', 379), ('y o u r </w>', 382), ('w </w>', 385), ('k n o w </w>', 388), (\"' ' [ </w>\", 434), ('t h i s </w>', 435), (\"' ' </w>\", 456), (': </w>', 500), ('. . . </w>', 543), ('i n </w>', 553), ('! </w>', 562), ('m e </w>', 567), ('i s </w>', 577), ('Y o u </w>', 582), (\"] ' ' </w>\", 641), ('t </w>', 671), ('o f </w>', 714), ('t h a t </w>', 772), ('i t </w>', 810), ('J e s s e </w>', 843), ('a n d </w>', 861), ('a </w>', 1046), ('s </w>', 1094), ('W a l t e r </w>', 1308), ('t o </w>', 1406), ('t h e </w>', 1507), ('? </w>', 1710), ('y o u </w>', 1815), ('I </w>', 2213), (\"' ' ' : </w>\", 2378), (\": ' ' ' </w>\", 2573), (\"' </w>\", 2722), (', </w>', 3930), ('. </w>', 4685)]\n"
     ]
    }
   ],
   "source": [
    "bpe_bb_vocab = create_bpe_vocab(breaking_bad_vocab)\n",
    "print(sorted(bpe_bb_vocab.items(), key=lambda x:x[1])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddcf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10007.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "for i in trange(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c3a2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a0fe108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {(':', \"'\"): 2573, (\"'\", \"'\"): 11433, (\"'\", '</w>'): 6392, ('w', '</w>'): 773, (':', '</w>'): 2878, ('a', '</w>'): 1046, ('-', '</w>'): 359, (',', '</w>'): 3930, ('W', 'a'): 1308, ('a', 'l'): 1308, ('l', 't'): 1308, ('t', 'e'): 1308, ('e', 'r'): 1615, ('r', '</w>'): 2367, ('t', 'h'): 2714, ('h', 'a'): 1151, ('a', 't'): 1151, ('t', '</w>'): 3268, ('h', 'e'): 1862, ('e', '</w>'): 3938, ('.', '</w>'): 5228, ('t', 'o'): 1406, ('o', '</w>'): 1755, ('o', 'f'): 714, ('f', '</w>'): 714, ('a', 'n'): 1173, ('n', 'd'): 861, ('d', '</w>'): 861, ('f', 'o'): 370, ('o', 'r'): 370, ('i', 's'): 1012, ('s', '</w>'): 2106, ('I', '</w>'): 2213, ('h', 'i'): 435, ('n', 'o'): 699, ('o', 't'): 311, ('m', 'y'): 363, ('y', '</w>'): 363, ('S', 'k'): 307, ('k', 'y'): 307, ('y', 'l'): 307, ('l', 'e'): 307, ('y', 'o'): 2197, ('o', 'u'): 2779, ('u', '</w>'): 2397, ('k', 'n'): 388, ('o', 'w'): 388, ('r', 'e'): 358, ('.', '.'): 1086, ('m', 'e'): 567, ('i', 'n'): 553, ('n', '</w>'): 900, ('j', 'u'): 325, ('u', 's'): 325, ('s', 't'): 325, ('i', 't'): 810, ('J', 'e'): 843, ('e', 's'): 843, ('s', 's'): 843, ('s', 'e'): 843, ('?', '</w>'): 1710, (\"'\", ':'): 2378, ('w', 'h'): 379, ('o', 'n'): 347, ('Y', 'o'): 582, ('u', 'r'): 382, ('m', '</w>'): 324, ('!', '</w>'): 562, (']', \"'\"): 641, (\"'\", '['): 434, ('[', '</w>'): 434, ('d', 'o'): 349, ('w', 'e'): 308, ('H', 'a'): 312, ('n', 'k'): 312, ('k', '</w>'): 312})\n"
     ]
    }
   ],
   "source": [
    "print(get_stats(bpe_bb_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1bae6c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'\", \"'\")\n"
     ]
    }
   ],
   "source": [
    "pairs = get_stats(bpe_bb_vocab)\n",
    "best = max(pairs, key=pairs.get)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "782823b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def merge_vocab(pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str,int]:\n",
    "    merged_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in vocab:\n",
    "        new_word = p.sub(''.join(pair), word)\n",
    "        merged_vocab[new_word] = vocab[word]\n",
    "    return merged_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1d16e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\": ' ' ' </w>\": 2573,\n",
       " 'w </w>': 385,\n",
       " ': </w>': 500,\n",
       " 'a </w>': 1046,\n",
       " '- </w>': 359,\n",
       " ', </w>': 3930,\n",
       " 'W a l t e r </w>': 1308,\n",
       " 't h a t </w>': 772,\n",
       " 'h e </w>': 355,\n",
       " '. </w>': 4685,\n",
       " 't o </w>': 1406,\n",
       " 'o f </w>': 714,\n",
       " 't h e </w>': 1507,\n",
       " 'a n d </w>': 861,\n",
       " 'f o r </w>': 370,\n",
       " 'i s </w>': 577,\n",
       " \"' </w>\": 2722,\n",
       " 's </w>': 1094,\n",
       " 'I </w>': 2213,\n",
       " 't h i s </w>': 435,\n",
       " 'n o t </w>': 311,\n",
       " 'm y </w>': 363,\n",
       " 'S k y l e r </w>': 307,\n",
       " 'y o u </w>': 1815,\n",
       " 'k n o w </w>': 388,\n",
       " 'r e </w>': 358,\n",
       " '. . . </w>': 543,\n",
       " 'm e </w>': 567,\n",
       " 'i n </w>': 553,\n",
       " 'j u s t </w>': 325,\n",
       " 'i t </w>': 810,\n",
       " 'J e s s e </w>': 843,\n",
       " '? </w>': 1710,\n",
       " \"' ' ' : </w>\": 2378,\n",
       " 't </w>': 671,\n",
       " 'w h a t </w>': 379,\n",
       " 'o n </w>': 347,\n",
       " 'Y o u </w>': 582,\n",
       " 'y o u r </w>': 382,\n",
       " 'm </w>': 324,\n",
       " '! </w>': 562,\n",
       " \"] ' ' </w>\": 641,\n",
       " \"' ' </w>\": 456,\n",
       " \"' ' [ </w>\": 434,\n",
       " 'd o </w>': 349,\n",
       " 'w e </w>': 308,\n",
       " 'H a n k </w>': 312}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_bb_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf060d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\": ' ' ' </w>\": 2573, 'w </w>': 385, ': </w>': 500, 'a </w>': 1046, '- </w>': 359, ', </w>': 3930, 'Wa l t e r </w>': 1308, 't h a t </w>': 772, 'h e </w>': 355, '. </w>': 4685, 't o </w>': 1406, 'o f </w>': 714, 't h e </w>': 1507, 'a n d </w>': 861, 'f o r </w>': 370, 'i s </w>': 577, \"' </w>\": 2722, 's </w>': 1094, 'I </w>': 2213, 't h i s </w>': 435, 'n o t </w>': 311, 'm y </w>': 363, 'S k y l e r </w>': 307, 'y o u </w>': 1815, 'k n o w </w>': 388, 'r e </w>': 358, '. . . </w>': 543, 'm e </w>': 567, 'i n </w>': 553, 'j u s t </w>': 325, 'i t </w>': 810, 'J e s s e </w>': 843, '? </w>': 1710, \"' ' ' : </w>\": 2378, 't </w>': 671, 'w h a t </w>': 379, 'o n </w>': 347, 'Y o u </w>': 582, 'y o u r </w>': 382, 'm </w>': 324, '! </w>': 562, \"] ' ' </w>\": 641, \"' ' </w>\": 456, \"' ' [ </w>\": 434, 'd o </w>': 349, 'w e </w>': 308, 'H a n k </w>': 312}\n"
     ]
    }
   ],
   "source": [
    "merged_bb_vocab = merge_vocab(('W', 'a'), bpe_bb_vocab)\n",
    "print(merged_bb_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4a0fef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bytes_freq(vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    byte_freq = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        bytes_ = word.split(' ')\n",
    "        for byte in bytes_:\n",
    "            byte_freq[byte] += 1\n",
    "    \n",
    "    for token in ['<line/>', '</line>', '<pad>', '<unk>']:\n",
    "        byte_freq[token] += 1\n",
    "    return byte_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "139dd300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {':': 3, \"'\": 13, '</w>': 47, 'w': 4, 'a': 5, '-': 1, ',': 1, 'Wa': 1, 'l': 2, 't': 11, 'e': 9, 'r': 5, 'h': 5, '.': 4, 'o': 10, 'f': 2, 'n': 6, 'd': 2, 'i': 4, 's': 6, 'I': 1, 'm': 3, 'y': 4, 'S': 1, 'k': 3, 'u': 4, 'j': 1, 'J': 1, '?': 1, 'Y': 1, '!': 1, ']': 1, '[': 1, 'H': 1, '<line/>': 1, '</line>': 1, '<pad>': 1, '<unk>': 1})\n"
     ]
    }
   ],
   "source": [
    "bytes_vocab = create_bytes_freq(merged_bb_vocab)\n",
    "print(bytes_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d1d1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_maps(vocab: Dict[str, int]) -> (Dict[str, int], Dict[int, str]):\n",
    "    ordered_freqs = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocab_to_idx, idx_to_vocab = {}, {}\n",
    "    for i in range(len(ordered_freqs)):\n",
    "        word, freq = ordered_freqs[i]\n",
    "        vocab_to_idx[word] = i\n",
    "        idx_to_vocab[i] = word\n",
    "    return vocab_to_idx, idx_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "721ce426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'</w>': 0, \"'\": 1, 't': 2, 'o': 3, 'e': 4, 'n': 5, 's': 6, 'a': 7, 'r': 8, 'h': 9, 'w': 10, '.': 11, 'i': 12, 'y': 13, 'u': 14, ':': 15, 'm': 16, 'k': 17, 'l': 18, 'f': 19, 'd': 20, '-': 21, ',': 22, 'Wa': 23, 'I': 24, 'S': 25, 'j': 26, 'J': 27, '?': 28, 'Y': 29, '!': 30, ']': 31, '[': 32, 'H': 33, '<line/>': 34, '</line>': 35, '<pad>': 36, '<unk>': 37} {0: '</w>', 1: \"'\", 2: 't', 3: 'o', 4: 'e', 5: 'n', 6: 's', 7: 'a', 8: 'r', 9: 'h', 10: 'w', 11: '.', 12: 'i', 13: 'y', 14: 'u', 15: ':', 16: 'm', 17: 'k', 18: 'l', 19: 'f', 20: 'd', 21: '-', 22: ',', 23: 'Wa', 24: 'I', 25: 'S', 26: 'j', 27: 'J', 28: '?', 29: 'Y', 30: '!', 31: ']', 32: '[', 33: 'H', 34: '<line/>', 35: '</line>', 36: '<pad>', 37: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "vocab_to_idx, idx_to_vocab = create_vocab_maps(bytes_vocab) \n",
    "print(vocab_to_idx, idx_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1c71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea4c34e1",
   "metadata": {},
   "source": [
    "### 10th January"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1d68b",
   "metadata": {},
   "source": [
    "Didn't do anything related to Tech stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a025b52",
   "metadata": {},
   "source": [
    "### 11th January\n",
    "\n",
    "Tried to understand the tokenize_dataset in GPT1 and understood few methods like merge_byte and other supporting methods in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57d3a0",
   "metadata": {},
   "source": [
    "## 12th January"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97171d",
   "metadata": {},
   "source": [
    "understanding the code related to tokenize_dataset\n",
    "\n",
    "Didn't get the time do a bit of coding as I was fully occupied with Kondal's discussion in the evening about life"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71466992",
   "metadata": {},
   "source": [
    "### 13th January"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83af426",
   "metadata": {},
   "source": [
    "started of the day with RNN's vanishing gradients problem.(because of the repeated multiflication of the same weight matrix in backward propagation as it was recurrent with same weight with different time steps(we calculate loss for the timesteps))\n",
    "\n",
    "how it was handled in the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea99ebe",
   "metadata": {},
   "source": [
    "### 14th January"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1cf77",
   "metadata": {},
   "source": [
    "Going through the interview and other process for RecVue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f727f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SnowBall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
